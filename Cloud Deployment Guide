# MI.Hub Cloud Deployment Guide

<!-- Generated by GithubCopilot -->

Complete guide for deploying MI.Hub microservices to Azure Kubernetes Service (AKS).

## Table of Contents

1. [Prerequisites](#1-prerequisites)
2. [Azure Infrastructure Overview](#2-azure-infrastructure-overview)
3. [Checking Current Status](#3-checking-current-status)
4. [Initial Setup](#4-initial-setup)
5. [Building Docker Images](#5-building-docker-images)
6. [Pushing Images to ACR](#6-pushing-images-to-acr)
7. [Database Migrations](#7-database-migrations)
8. [Configuring Secrets](#8-configuring-secrets)
9. [Deploying to Kubernetes](#9-deploying-to-kubernetes)
10. [Verification](#10-verification)
11. [Troubleshooting](#11-troubleshooting)
12. [Service Reference](#12-service-reference)
13. [Automation Scripts](#13-automation-scripts)

---

## Understanding the Deployment Flow

### Why This Order Matters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DEPLOYMENT DEPENDENCY FLOW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. LOGIN TO AZURE & ACR                                                    â”‚
â”‚     â””â”€â”€ WHY: You need authentication to access cloud resources              â”‚
â”‚              Without this, you can't push images or deploy                  â”‚
â”‚                                                                             â”‚
â”‚  2. BUILD DOCKER IMAGES                                                     â”‚
â”‚     â””â”€â”€ WHY: Kubernetes pulls images from a registry, not local files       â”‚
â”‚              The code must be packaged into container images first          â”‚
â”‚                                                                             â”‚
â”‚  3. PUSH IMAGES TO ACR                                                      â”‚
â”‚     â””â”€â”€ WHY: AKS can only pull images from ACR (or other registries)        â”‚
â”‚              Local images on your machine are not accessible to AKS         â”‚
â”‚                                                                             â”‚
â”‚  4. RUN DATABASE MIGRATIONS                                                 â”‚
â”‚     â””â”€â”€ WHY: Applications expect database tables to exist                   â”‚
â”‚              If tables don't exist, apps will crash on startup              â”‚
â”‚                                                                             â”‚
â”‚  5. CONFIGURE SECRETS                                                       â”‚
â”‚     â””â”€â”€ WHY: Apps need credentials (DB password, API keys)                  â”‚
â”‚              Without secrets, pods can't connect to databases               â”‚
â”‚                                                                             â”‚
â”‚  6. DEPLOY TO KUBERNETES                                                    â”‚
â”‚     â””â”€â”€ WHY: Now everything is ready:                                       â”‚
â”‚              - Images exist in ACR âœ“                                        â”‚
â”‚              - Database has tables âœ“                                        â”‚
â”‚              - Secrets are configured âœ“                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Dependency Order

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WHY DEPLOY IN THIS ORDER?                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  LAYER 1: Core Backend (Deploy First)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Plant, UserManagement, AgenticSystem, AgenticScheduling            â”‚   â”‚
â”‚  â”‚  WHY FIRST: These provide base APIs that other services call        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“ depends on                                   â”‚
â”‚  LAYER 2: Data Services                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DataProductHub, SchedulingEngineCore                               â”‚   â”‚
â”‚  â”‚  WHY: Scheduler & Workforce apps query data from these services     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“ depends on                                   â”‚
â”‚  LAYER 3: AI Services                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AgentsRuntime, AgentsCoreServices                                  â”‚   â”‚
â”‚  â”‚  WHY: Portal and apps use AI features from these services           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“ depends on                                   â”‚
â”‚  LAYER 4: Application Services                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AgentsPortal, Scheduler, WorkforceOptimizer                        â”‚   â”‚
â”‚  â”‚  WHY: These call all the above services                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“ depends on                                   â”‚
â”‚  LAYER 5: Frontend (Deploy Last)                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  WebPortal                                                          â”‚   â”‚
â”‚  â”‚  WHY LAST: UI needs all backend services to be running              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Where to Execute Commands

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      COMMAND EXECUTION LOCATIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  YOUR LOCAL MACHINE (WSL/Linux Terminal)                                    â”‚
â”‚  â”œâ”€â”€ az login                    # Azure CLI commands                       â”‚
â”‚  â”œâ”€â”€ docker build / push         # Building and pushing images              â”‚
â”‚  â”œâ”€â”€ kubectl apply               # Deploying to Kubernetes                  â”‚
â”‚  â””â”€â”€ ./scripts/deploy-all.sh     # Running automation scripts               â”‚
â”‚                                                                             â”‚
â”‚  WORKSPACE ROOT DIRECTORY: /home/ivimehaj/projects/Ariston/MI.SA            â”‚
â”‚  â””â”€â”€ Most commands should be run from here because:                         â”‚
â”‚      - Docker build contexts reference relative paths                       â”‚
â”‚      - Scripts use relative paths to find kube/ directories                 â”‚
â”‚      - Kustomize references ../base from development/ folder                â”‚
â”‚                                                                             â”‚
â”‚  INSIDE KUBERNETES PODS (for debugging)                                     â”‚
â”‚  â””â”€â”€ kubectl exec -it <pod-name> -- /bin/bash                               â”‚
â”‚      Use this to debug inside running containers                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Prerequisites

### Required Tools

```bash
# Azure CLI
az --version  # >= 2.50.0

# kubectl
kubectl version --client  # >= 1.28

# Docker
docker --version  # >= 24.0

# Kustomize (optional, kubectl has built-in support)
kustomize version  # >= 5.0
```

### Install Missing Tools

```bash
# Azure CLI (Linux)
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# kubectl
az aks install-cli

# Docker
# Follow: https://docs.docker.com/engine/install/
```

### Required Access

- Azure subscription with Contributor role
- Access to Azure Container Registry (ACR)
- Access to AKS cluster
- PostgreSQL database credentials
- Azure OpenAI API key (for AI services)

---

## 2. Azure Infrastructure Overview

### Resource Groups

| Resource Group | Purpose |
|----------------|---------|
| `rg-31314402-UKS-mihazks` | AKS cluster and networking |
| `rg-31314402-UKS-mihub` | Application resources (databases, storage, etc.) |

### Key Resources

| Resource | Name | Purpose |
|----------|------|---------|
| AKS Cluster | `aksnpdmih` | Kubernetes cluster |
| Container Registry | `acr31314402uksnpdmihacr` | Docker image repository |
| PostgreSQL (Primary) | `gmcs-313144-npdmihpgsql001-sql-npd.postgres.database.azure.com` | Databases for all services |
| PostgreSQL (Secondary) | `gmcs-313144-npdmihpgsql002-sql-npd.postgres.database.azure.com` | Backup/failover database |
| Storage Account | `st31314402uksnpdmih` | Blob storage for artifacts |
| Key Vault | `kv31314402uksnpdmihkv` | Secrets management |
| Azure OpenAI | `mih313144npdai` | AI/LLM capabilities |

> **Note:** PostgreSQL servers are in resource group `rg313144mihuksnpdaria`
> **Note:** Key Vault is in resource group `rg313144mihuksnpdaria`
> **Note:** Azure OpenAI is in resource group `rg-mih313144npdopenai`

### Kubernetes Namespace

All services deploy to: `mi-hub-dev`

---

## 3. Checking Current Status

Before deploying, check what already exists in your Azure environment.

### Where to Run These Commands

```bash
# All commands run from your LOCAL TERMINAL (WSL/Linux)
# Make sure you're logged in first: az login
```

### Check ACR (Azure Container Registry)

```bash
# List all repositories (images) in ACR
az acr repository list --name acr31314402uksnpdmihacr --output table

# Example output:
# Result
# ----------------------
# scheduler-backend
# scheduler-frontend
# web-portal

# Check tags for a specific image
az acr repository show-tags \
  --name acr31314402uksnpdmihacr \
  --repository scheduler-backend \
  --output table

# Example output:
# Result
# --------
# latest
# v1.0.0
# abc123

# Check if a specific image exists
az acr repository show \
  --name acr31314402uksnpdmihacr \
  --image scheduler-backend:latest

# Check ACR login status
az acr login --name acr31314402uksnpdmihacr --expose-token
```

### Check AKS Cluster Status

```bash
# Check cluster is running
az aks show \
  --resource-group rg-31314402-UKS-mihazks \
  --name aksnpdmih \
  --query "provisioningState" -o tsv
# Should output: Succeeded

# Check node status
kubectl get nodes
# Example output:
# NAME                              STATUS   ROLES   AGE   VERSION
# aks-nodepool1-12345678-vmss0000   Ready    agent   30d   v1.28.5

# Check what's already deployed
kubectl get all -n mi-hub-dev

# Check namespace exists
kubectl get namespace mi-hub-dev
```

### Check PostgreSQL Database

```bash
# Check database server status (Primary)
az postgres flexible-server show \
  --resource-group rg313144mihuksnpdaria \
  --name gmcs-313144-npdmihpgsql001-sql-npd \
  --query "state" -o tsv
# Should output: Ready

# Check database server status (Secondary)
az postgres flexible-server show \
  --resource-group rg313144mihuksnpdaria \
  --name gmcs-313144-npdmihpgsql002-sql-npd \
  --query "state" -o tsv

# List databases on the server
az postgres flexible-server db list \
  --resource-group rg313144mihuksnpdaria \
  --server-name gmcs-313144-npdmihpgsql001-sql-npd \
  --output table
```

### Check Key Vault Secrets

```bash
# List available secrets
az keyvault secret list \
  --vault-name kv31314402uksnpdmihkv \
  --output table

# Check if a specific secret exists
az keyvault secret show \
  --vault-name kv31314402uksnpdmihkv \
  --name postgres-admin-password \
  --query "name" -o tsv
```

### Check Azure OpenAI

```bash
# Check OpenAI resource status
az cognitiveservices account show \
  --resource-group rg-mih313144npdopenai \
  --name mih313144npdai \
  --query "properties.provisioningState" -o tsv
# Should output: Succeeded

# List deployed models
az cognitiveservices account deployment list \
  --resource-group rg-mih313144npdopenai \
  --name mih313144npdai \
  --output table
```

### Quick Status Check Script

```bash
#!/bin/bash
# Save as: scripts/check-status.sh
# Run from workspace root: ./scripts/check-status.sh

echo "============================================"
echo "MI.Hub Infrastructure Status Check"
echo "============================================"

echo -e "\nğŸ“¦ ACR Images:"
az acr repository list --name acr31314402uksnpdmihacr --output table 2>/dev/null || echo "âŒ Cannot access ACR"

echo -e "\nğŸ¯ AKS Cluster:"
kubectl get nodes 2>/dev/null || echo "âŒ Cannot access AKS"

echo -e "\nğŸ“Š Deployed Pods:"
kubectl get pods -n mi-hub-dev 2>/dev/null || echo "âŒ Namespace not found or empty"

echo -e "\nğŸ” Key Vault Secrets:"
az keyvault secret list --vault-name kv31314402uksnpdmihkv --query "[].name" -o tsv 2>/dev/null || echo "âŒ Cannot access Key Vault"

echo -e "\nâœ… Status check complete"
```

---

## 4. Initial Setup

> **WHY THIS STEP?** You must authenticate with Azure before you can access any resources.
> Without login, commands like `docker push` or `kubectl apply` will fail with "unauthorized".

### Step 4.1: Login to Azure

```bash
# Interactive login
az login

# Set subscription (if you have multiple)
az account set --subscription "YOUR_SUBSCRIPTION_ID"

# Verify
az account show
```

### Step 4.2: Connect to AKS Cluster

> **WHY?** kubectl needs credentials to talk to your specific AKS cluster.
> This downloads a certificate and configures kubectl to use it.

```bash
# Get AKS credentials
az aks get-credentials \
  --resource-group rg-31314402-UKS-mihazks \
  --name aksnpdmih

# Verify connection
kubectl cluster-info
kubectl get nodes
```

### Step 4.3: Login to Azure Container Registry

> **WHY?** Docker needs permission to push images to ACR.
> This command gets a token that allows `docker push` to work.

```bash
# Login to ACR
az acr login --name acr31314402uksnpdmihacr

# Verify login
docker login acr31314402uksnpdmihacr.azurecr.io
# Should say "Login Succeeded"
```

### Step 4.4: Create Namespace (if not exists)

> **WHY?** Namespaces isolate resources. All our services go into `mi-hub-dev`.
> This keeps them separate from other applications in the cluster.

```bash
kubectl create namespace mi-hub-dev
kubectl config set-context --current --namespace=mi-hub-dev
```

---

## 5. Building Docker Images

> **WHY THIS STEP?** Kubernetes doesn't run code directly - it runs container images.
> Building creates a packaged version of your application that can run anywhere.
>
> **WHERE TO RUN:** From workspace root `/home/ivimehaj/projects/Ariston/MI.SA`
> because Dockerfiles reference paths relative to their context.

### Service Image Reference

| Service | Image Name | Dockerfile Location |
|---------|------------|---------------------|
| Scheduler Backend | `scheduler-backend` | `MI.SA.Scheduler/backend/Dockerfile` |
| Scheduler Frontend | `scheduler-frontend` | `MI.SA.Scheduler/frontend/Dockerfile.dev` |
| Workforce Backend | `workforce-optimizer-backend` | `MI.SA.WorkforceOptimizer/backend/Dockerfile` |
| Workforce Frontend | `workforce-optimizer-frontend` | `MI.SA.WorkforceOptimizer/frontend/Dockerfile.dev` |
| Web Portal | `web-portal` | `MI.WebPortal/frontend/Dockerfile` |
| Agents Portal Backend | `agents-portal-backend` | `MI.AgentsPortal/backend/Dockerfile` |
| Agents Portal UI | `agents-portal-ui` | `MI.AgentsPortal/ui/Dockerfile` |
| Agents Runtime | `agents-runtime` | `MI.AgentsRuntime/Dockerfile.example` |
| Agents Service | `agents-service` | `MI.AgentsCoreServices/services/agents_service/Dockerfile` |
| Artifacts Service | `artifacts-service` | `MI.AgentsCoreServices/services/artifacts_service/Dockerfile` |
| Knowledge Service | `knowledge-service` | `MI.AgentsCoreServices/services/knowledge_service/Dockerfile` |
| Scheduling Engine Core | `scheduling-engine-core` | `MI.SA.SchedulingEngineCore/Dockerfile` |

### Build Commands

```bash
# Set ACR name
ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="latest"  # Or use git commit SHA / version

# Navigate to workspace root
cd /home/ivimehaj/projects/Ariston/MI.SA

# ============================================================================
# MI.SA.Scheduler
# ============================================================================
docker build -t ${ACR_NAME}/scheduler-backend:${TAG} \
  -f MI.SA.Scheduler/backend/Dockerfile \
  MI.SA.Scheduler/backend

docker build -t ${ACR_NAME}/scheduler-frontend:${TAG} \
  -f MI.SA.Scheduler/frontend/Dockerfile.dev \
  MI.SA.Scheduler/frontend

# ============================================================================
# MI.SA.WorkforceOptimizer
# ============================================================================
docker build -t ${ACR_NAME}/workforce-optimizer-backend:${TAG} \
  -f MI.SA.WorkforceOptimizer/backend/Dockerfile \
  MI.SA.WorkforceOptimizer/backend

docker build -t ${ACR_NAME}/workforce-optimizer-frontend:${TAG} \
  -f MI.SA.WorkforceOptimizer/frontend/Dockerfile.dev \
  MI.SA.WorkforceOptimizer/frontend

# ============================================================================
# MI.WebPortal
# ============================================================================
docker build -t ${ACR_NAME}/web-portal:${TAG} \
  -f MI.WebPortal/frontend/Dockerfile \
  MI.WebPortal/frontend

# ============================================================================
# MI.AgentsPortal
# ============================================================================
docker build -t ${ACR_NAME}/agents-portal-backend:${TAG} \
  -f MI.AgentsPortal/backend/Dockerfile \
  MI.AgentsPortal/backend

docker build -t ${ACR_NAME}/agents-portal-ui:${TAG} \
  -f MI.AgentsPortal/ui/Dockerfile \
  MI.AgentsPortal/ui

# ============================================================================
# MI.AgentsRuntime
# ============================================================================
docker build -t ${ACR_NAME}/agents-runtime:${TAG} \
  -f MI.AgentsRuntime/Dockerfile.example \
  MI.AgentsRuntime

# ============================================================================
# MI.AgentsCoreServices (3 services)
# ============================================================================
docker build -t ${ACR_NAME}/agents-service:${TAG} \
  -f MI.AgentsCoreServices/services/agents_service/Dockerfile \
  .

docker build -t ${ACR_NAME}/artifacts-service:${TAG} \
  -f MI.AgentsCoreServices/services/artifacts_service/Dockerfile \
  .

docker build -t ${ACR_NAME}/knowledge-service:${TAG} \
  -f MI.AgentsCoreServices/services/knowledge_service/Dockerfile \
  .

# ============================================================================
# MI.SA.SchedulingEngineCore
# ============================================================================
docker build -t ${ACR_NAME}/scheduling-engine-core:${TAG} \
  -f MI.SA.SchedulingEngineCore/Dockerfile \
  MI.SA.SchedulingEngineCore
```

### Build Script (All Services)

Create `scripts/build-all-images.sh`:

```bash
#!/bin/bash
set -e

ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="${1:-latest}"

echo "Building all images with tag: ${TAG}"

# Array of builds: "image_name:dockerfile_path:context_path"
BUILDS=(
  "scheduler-backend:MI.SA.Scheduler/backend/Dockerfile:MI.SA.Scheduler/backend"
  "scheduler-frontend:MI.SA.Scheduler/frontend/Dockerfile.dev:MI.SA.Scheduler/frontend"
  "workforce-optimizer-backend:MI.SA.WorkforceOptimizer/backend/Dockerfile:MI.SA.WorkforceOptimizer/backend"
  "workforce-optimizer-frontend:MI.SA.WorkforceOptimizer/frontend/Dockerfile.dev:MI.SA.WorkforceOptimizer/frontend"
  "web-portal:MI.WebPortal/frontend/Dockerfile:MI.WebPortal/frontend"
  "agents-portal-backend:MI.AgentsPortal/backend/Dockerfile:MI.AgentsPortal/backend"
  "agents-portal-ui:MI.AgentsPortal/ui/Dockerfile:MI.AgentsPortal/ui"
  "agents-runtime:MI.AgentsRuntime/Dockerfile.example:MI.AgentsRuntime"
  "scheduling-engine-core:MI.SA.SchedulingEngineCore/Dockerfile:MI.SA.SchedulingEngineCore"
)

for build in "${BUILDS[@]}"; do
  IFS=':' read -r name dockerfile context <<< "$build"
  echo "========================================="
  echo "Building: ${name}"
  echo "========================================="
  docker build -t ${ACR_NAME}/${name}:${TAG} -f ${dockerfile} ${context}
done

# CoreServices (need root context)
echo "Building CoreServices..."
docker build -t ${ACR_NAME}/agents-service:${TAG} -f MI.AgentsCoreServices/services/agents_service/Dockerfile .
docker build -t ${ACR_NAME}/artifacts-service:${TAG} -f MI.AgentsCoreServices/services/artifacts_service/Dockerfile .
docker build -t ${ACR_NAME}/knowledge-service:${TAG} -f MI.AgentsCoreServices/services/knowledge_service/Dockerfile .

echo "All images built successfully!"
```

---

## 6. Pushing Images to ACR

> **WHY THIS STEP?** Images on your local machine are not accessible to AKS.
> AKS pulls images from ACR. If the image doesn't exist in ACR, pods fail with `ImagePullBackOff`.
>
> **PREREQUISITE:** You must have run `az acr login` first (Step 4.3)

### Push Commands

```bash
ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="latest"

# Push all images
docker push ${ACR_NAME}/scheduler-backend:${TAG}
docker push ${ACR_NAME}/scheduler-frontend:${TAG}
docker push ${ACR_NAME}/workforce-optimizer-backend:${TAG}
docker push ${ACR_NAME}/workforce-optimizer-frontend:${TAG}
docker push ${ACR_NAME}/web-portal:${TAG}
docker push ${ACR_NAME}/agents-portal-backend:${TAG}
docker push ${ACR_NAME}/agents-portal-ui:${TAG}
docker push ${ACR_NAME}/agents-runtime:${TAG}
docker push ${ACR_NAME}/agents-service:${TAG}
docker push ${ACR_NAME}/artifacts-service:${TAG}
docker push ${ACR_NAME}/knowledge-service:${TAG}
docker push ${ACR_NAME}/scheduling-engine-core:${TAG}
```

### Push Script

Create `scripts/push-all-images.sh`:

```bash
#!/bin/bash
set -e

ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="${1:-latest}"

IMAGES=(
  "scheduler-backend"
  "scheduler-frontend"
  "workforce-optimizer-backend"
  "workforce-optimizer-frontend"
  "web-portal"
  "agents-portal-backend"
  "agents-portal-ui"
  "agents-runtime"
  "agents-service"
  "artifacts-service"
  "knowledge-service"
  "scheduling-engine-core"
)

echo "Pushing all images with tag: ${TAG}"

for image in "${IMAGES[@]}"; do
  echo "Pushing: ${image}"
  docker push ${ACR_NAME}/${image}:${TAG}
done

echo "All images pushed successfully!"
```

### Alternative: Build and Push with ACR Tasks

```bash
# Build and push directly in ACR (no local Docker required)
az acr build \
  --registry acr31314402uksnpdmihacr \
  --image scheduler-backend:latest \
  --file MI.SA.Scheduler/backend/Dockerfile \
  MI.SA.Scheduler/backend
```

---

## 7. Database Migrations

> **WHY THIS STEP?** Applications expect database tables to exist. If you deploy an app
> before running migrations, it will crash with errors like "relation does not exist".
>
> **ARE MIGRATIONS INCLUDED IN DOCKERFILE?** It depends on the service:
>
> | Service Type | Migration Approach |
> |--------------|-------------------|
> | **Scheduler/Workforce** (aTemplateAnagram) | YES - runs on startup via `--run-migrations` flag in CMD |
> | **GenAIX services** (AgentsPortal, Runtime, CoreServices) | NO - requires separate migration container |
> | **Backend services** (Plant, UserManagement) | YES - runs on startup |

### Understanding Migration Approaches

#### Approach 1: Migrations in Dockerfile (Automatic)

Services like Scheduler, Workforce, Plant have migrations built into startup:

```dockerfile
# From MI.SA.Scheduler/backend/Dockerfile
CMD ["run", "backend", "--run-migrations"]
```

**What happens:**
1. Pod starts
2. Before serving requests, it runs Alembic migrations
3. Then starts the web server

**Pros:** Simple, automatic
**Cons:** First pod startup is slower; multiple replicas may race

#### Approach 2: Separate Migration Job (Manual)

GenAIX services (AgentsPortal, CoreServices) need migrations run separately:

```bash
# Each has a migrations/Dockerfile that runs Alembic only
# Example: MI.AgentsPortal/migrations/Dockerfile
```

**What happens:**
1. Run migration container first (one-time)
2. Migration container exits after success
3. Then deploy the application

### Running Migrations for GenAIX Services

```bash
# Navigate to workspace root
cd /home/ivimehaj/projects/Ariston/MI.SA

# ============================================================================
# Option 1: Run migrations via docker-compose (local dev)
# ============================================================================
cd MI.AgentsPortal
docker-compose run --rm portal_db_migrate

cd MI.AgentsCoreServices
docker-compose run --rm genaix_runtime_migrate
docker-compose run --rm knowledge_migrate
docker-compose run --rm artifacts_migrate

# ============================================================================
# Option 2: Run migrations as Kubernetes Job (production)
# ============================================================================
# Create a migration job manifest and apply it
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: agents-portal-migrate
  namespace: mi-hub-dev
spec:
  template:
    spec:
      containers:
      - name: migrate
        image: acr31314402uksnpdmihacr.azurecr.io/agents-portal-backend:latest
        command: ["alembic", "upgrade", "head"]
        envFrom:
        - secretRef:
            name: agents-portal-backend-secrets
      restartPolicy: Never
  backoffLimit: 3
EOF

# Check migration job status
kubectl get jobs -n mi-hub-dev
kubectl logs job/agents-portal-migrate -n mi-hub-dev
```

### Migration Order

Run migrations in this order (matches service dependencies):

```bash
# 1. Core backends (have auto-migrations, but verify DB exists)
#    - mi_plant, mi_user_management databases

# 2. Data services
#    - mi_dph (DataProductHub)
#    - scheduling_engine_core

# 3. GenAIX services (MUST run manually)
#    - genaix_portal
#    - genaix_agents, genaix_artifacts, genaix_knowledge (CoreServices)
#    - genaix_runtime

# 4. Application services (auto-migrate on startup)
#    - mi_scheduler
#    - mi_workforce_optimizer
```

### Verifying Migrations

```bash
# Connect to a pod and check database
kubectl exec -it deploy/scheduler-backend -n mi-hub-dev -- \
  python -c "from sqlalchemy import create_engine; e = create_engine('$DATABASE_URL'); print(e.execute('SELECT version_num FROM alembic_version').fetchone())"

# Or run psql directly
kubectl run psql-check --rm -it --image=postgres:14 -n mi-hub-dev -- \
  psql "postgresql://USER:PASS@gmcs-313144-npdmihpgsql001-sql-npd.postgres.database.azure.com:5432/mi_scheduler?sslmode=require" \
  -c "SELECT * FROM alembic_version;"
```

---

## 8. Configuring Secrets

> **WHY THIS STEP?** Pods need credentials to connect to databases, APIs, etc.
> Without proper secrets, pods will crash with "connection refused" or "authentication failed".

### Step 8.1: Get Database Credentials

```bash
# From Azure Key Vault
az keyvault secret show \
  --vault-name kv31314402uksnpdmihkv \
  --name postgres-admin-password \
  --query value -o tsv
```

### Step 8.2: Update Secrets Files

Each service has a `kube/development/secrets.yaml` file. Replace placeholders:

```yaml
# Example: MI.SA.Scheduler/kube/development/secrets.yaml
stringData:
  DATABASE_USER: "mi_admin"                    # Replace
  DATABASE_PASSWORD: "ActualPassword123!"      # Replace
  POSTGRES_DSN: "postgresql://mi_admin:ActualPassword123!@gmcs-313144-npdmihpgsql001-sql-npd.postgres.database.azure.com:5432/mi_scheduler?sslmode=require"
```

### Secret Values to Configure

| Secret | Where to Get |
|--------|--------------|
| `DATABASE_USER` | Azure portal or Key Vault |
| `DATABASE_PASSWORD` | Azure Key Vault |
| `AZURE_OPENAI_API_KEY` | Azure portal â†’ Azure OpenAI â†’ Keys |
| `KEYCLOAK_CLIENT_SECRET` | Keycloak Admin Console |
| `LANGFUSE_PUBLIC_KEY` | Langfuse dashboard (optional) |
| `LANGFUSE_SECRET_KEY` | Langfuse dashboard (optional) |
| `AZURE_STORAGE_CONNECTION_STRING` | Azure portal â†’ Storage Account â†’ Access Keys |
| `AZURE_SEARCH_API_KEY` | Azure portal â†’ AI Search â†’ Keys |

### Step 8.3: Create Kubernetes Secrets from Key Vault (Recommended)

```bash
# Using External Secrets Operator (if installed)
# Or manually:
kubectl create secret generic db-credentials \
  --from-literal=username=mi_admin \
  --from-literal=password=$(az keyvault secret show --vault-name kv31314402uksnpdmihkv --name postgres-admin-password --query value -o tsv) \
  -n mi-hub-dev
```

---

## 9. Deploying to Kubernetes

> **WHY THIS STEP?** This is where your application actually starts running in the cloud.
> `kubectl apply` sends your manifests to AKS, which schedules pods on nodes.
>
> **TWO APPROACHES:**
> - **Manual (one-by-one):** Good for debugging, seeing each service start
> - **Script (all at once):** Good for consistent, repeatable deployments

### Manual vs Script Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MANUAL vs SCRIPT DEPLOYMENT                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  MANUAL (One-by-One)                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  kubectl apply -k MI.Backend.Plant/kube/development                 â”‚   â”‚
â”‚  â”‚  kubectl apply -k MI.Backend.UserManagement/kube/development        â”‚   â”‚
â”‚  â”‚  # ... repeat for each service                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  âœ… PROS: See each step, easy to debug, stop if something fails           â”‚
â”‚  âŒ CONS: Tedious, easy to miss a service, inconsistent ordering          â”‚
â”‚                                                                             â”‚
â”‚  SCRIPT (All at Once)                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ./scripts/deploy-all.sh                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  âœ… PROS: Consistent, repeatable, correct order, one command              â”‚
â”‚  âŒ CONS: Harder to debug if something fails midway                       â”‚
â”‚                                                                             â”‚
â”‚  RECOMMENDATION: Use manual for first deployment, script for updates       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Order

Deploy services in this order to respect dependencies:

1. **Infrastructure** (databases, kafka, keycloak - if not using managed services)
2. **Core Backend Services**
   - MI.Backend.Plant
   - MI.Backend.UserManagement
   - MI.Backend.AgenticSystem
   - MI.Backend.AgenticScheduling
3. **Data Services**
   - MI.DataProductHub
   - MI.SA.SchedulingEngineCore
4. **AI Services**
   - MI.AgentsRuntime
   - MI.AgentsCoreServices (agents, artifacts, knowledge)
5. **Application Services**
   - MI.AgentsPortal
   - MI.SA.Scheduler
   - MI.SA.WorkforceOptimizer
6. **Frontend**
   - MI.WebPortal

### Deploy Commands

```bash
cd /home/ivimehaj/projects/Ariston/MI.SA

# ============================================================================
# Step 1: Preview what will be deployed (dry-run)
# ============================================================================
kubectl kustomize MI.Backend.Plant/kube/development
kubectl kustomize MI.Backend.UserManagement/kube/development
# ... etc

# ============================================================================
# Step 2: Deploy Core Backend Services
# ============================================================================
kubectl apply -k MI.Backend.Plant/kube/development
kubectl apply -k MI.Backend.UserManagement/kube/development
kubectl apply -k MI.Backend.AgenticSystem/kube/development
kubectl apply -k MI.Backend.AgenticScheduling/kube/development

# Wait for pods to be ready
kubectl wait --for=condition=ready pod -l app=plant -n mi-hub-dev --timeout=120s
kubectl wait --for=condition=ready pod -l app=user-management -n mi-hub-dev --timeout=120s

# ============================================================================
# Step 3: Deploy Data Services
# ============================================================================
kubectl apply -k MI.DataProductHub/kube/development
kubectl apply -k MI.SA.SchedulingEngineCore/kube/development

# ============================================================================
# Step 4: Deploy AI Services
# ============================================================================
kubectl apply -k MI.AgentsRuntime/kube/development
kubectl apply -k MI.AgentsCoreServices/kube/development

# ============================================================================
# Step 5: Deploy Application Services
# ============================================================================
kubectl apply -k MI.AgentsPortal/kube/development
kubectl apply -k MI.SA.Scheduler/kube/development
kubectl apply -k MI.SA.WorkforceOptimizer/kube/development

# ============================================================================
# Step 6: Deploy Frontend
# ============================================================================
kubectl apply -k MI.WebPortal/kube/development
```

### Deploy All Script

Create `scripts/deploy-all.sh`:

```bash
#!/bin/bash
set -e

NAMESPACE="mi-hub-dev"
ROOT_DIR="/home/ivimehaj/projects/Ariston/MI.SA"

echo "Deploying all services to namespace: ${NAMESPACE}"

# Services in dependency order
SERVICES=(
  "MI.Backend.Plant"
  "MI.Backend.UserManagement"
  "MI.Backend.AgenticSystem"
  "MI.Backend.AgenticScheduling"
  "MI.DataProductHub"
  "MI.SA.SchedulingEngineCore"
  "MI.AgentsRuntime"
  "MI.AgentsCoreServices"
  "MI.AgentsPortal"
  "MI.SA.Scheduler"
  "MI.SA.WorkforceOptimizer"
  "MI.WebPortal"
)

for service in "${SERVICES[@]}"; do
  KUBE_DIR="${ROOT_DIR}/${service}/kube/development"
  if [ -d "$KUBE_DIR" ]; then
    echo "========================================="
    echo "Deploying: ${service}"
    echo "========================================="
    kubectl apply -k ${KUBE_DIR}
    sleep 5  # Brief pause between deployments
  else
    echo "SKIP: ${service} (no kube directory)"
  fi
done

echo "All services deployed!"
echo "Run: kubectl get pods -n ${NAMESPACE} to check status"
```

---

## 10. Verification

> **WHY THIS STEP?** Deployment succeeds doesn't mean the app works.
> You need to verify pods are running and services are accessible.

### Check Pod Status

```bash
# All pods in namespace
kubectl get pods -n mi-hub-dev

# Watch pods come up
kubectl get pods -n mi-hub-dev -w

# Detailed pod info
kubectl describe pod <pod-name> -n mi-hub-dev
```

### Check Services

```bash
kubectl get svc -n mi-hub-dev
```

### Check Ingress

```bash
kubectl get ingress -n mi-hub-dev
```

### Check Logs

```bash
# Specific pod logs
kubectl logs <pod-name> -n mi-hub-dev

# Follow logs
kubectl logs -f <pod-name> -n mi-hub-dev

# All pods for a service
kubectl logs -l app=scheduler-backend -n mi-hub-dev
```

### Health Check Endpoints

```bash
# Port-forward to test locally
kubectl port-forward svc/scheduler-backend 8000:8000 -n mi-hub-dev

# Then test
curl http://localhost:8000/api/health
```

### Expected Healthy State

```
NAME                                      READY   STATUS    RESTARTS   AGE
scheduler-backend-xxx                     1/1     Running   0          5m
scheduler-frontend-xxx                    1/1     Running   0          5m
workforce-optimizer-backend-xxx           1/1     Running   0          5m
workforce-optimizer-frontend-xxx          1/1     Running   0          5m
web-portal-xxx                            1/1     Running   0          5m
agents-portal-backend-xxx                 1/1     Running   0          5m
agents-portal-ui-xxx                      1/1     Running   0          5m
agents-runtime-xxx                        1/1     Running   0          5m
agents-service-xxx                        1/1     Running   0          5m
artifacts-service-xxx                     1/1     Running   0          5m
knowledge-service-xxx                     1/1     Running   0          5m
scheduling-engine-core-xxx                1/1     Running   0          5m
```

---

## 11. Troubleshooting

> Common issues and how to fix them.

### Pod Not Starting

```bash
# Check pod events
kubectl describe pod <pod-name> -n mi-hub-dev

# Common issues:
# - ImagePullBackOff: Image doesn't exist in ACR or wrong name
# - CrashLoopBackOff: Application error, check logs
# - Pending: Not enough resources or node issues
```

### ImagePullBackOff

```bash
# Verify image exists in ACR
az acr repository list --name acr31314402uksnpdmihacr --output table
az acr repository show-tags --name acr31314402uksnpdmihacr --repository scheduler-backend

# Check AKS has access to ACR
az aks check-acr --name aksnpdmih \
  --resource-group rg-31314402-UKS-mihazks \
  --acr acr31314402uksnpdmihacr.azurecr.io
```

### CrashLoopBackOff

```bash
# Check logs
kubectl logs <pod-name> -n mi-hub-dev --previous

# Common causes:
# - Database connection failed (wrong credentials/host)
# - Missing environment variables
# - Application error on startup
```

### Database Connection Issues

```bash
# Test from a pod
kubectl run psql-test --rm -it --image=postgres:14 -n mi-hub-dev -- \
  psql "postgresql://USER:PASS@gmcs-313144-npdmihpgsql001-sql-npd.postgres.database.azure.com:5432/mi_scheduler?sslmode=require"
```

### Service Not Accessible

```bash
# Check service endpoints
kubectl get endpoints <service-name> -n mi-hub-dev

# Check ingress controller logs
kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx
```

### Rollback Deployment

```bash
# View rollout history
kubectl rollout history deployment/<deployment-name> -n mi-hub-dev

# Rollback to previous version
kubectl rollout undo deployment/<deployment-name> -n mi-hub-dev

# Rollback to specific revision
kubectl rollout undo deployment/<deployment-name> --to-revision=2 -n mi-hub-dev
```

---

## 12. Service Reference

### Service URLs (Internal - Kubernetes DNS)

| Service | Internal URL |
|---------|--------------|
| Plant | `http://plant:8000` |
| User Management | `http://user-management:8000` |
| Agentic Scheduling | `http://agentic-scheduling:8000` |
| Agentic System | `http://agentic-system:8000` |
| Data Product Hub | `http://dph:6001` |
| Scheduling Engine Core | `http://scheduling-engine-core:80` |
| Agents Runtime | `http://agents-runtime:80` |
| Agents Service | `http://agents-service:80` |
| Artifacts Service | `http://artifacts-service:80` |
| Knowledge Service | `http://knowledge-service:80` |
| Portal Backend | `http://agents-portal-backend:80` |
| Portal UI | `http://agents-portal-ui:443` |
| Scheduler Backend | `http://scheduler-backend:8000` |
| Scheduler Frontend | `http://scheduler-frontend:3000` |
| Workforce Backend | `http://workforce-optimizer-backend:8000` |
| Workforce Frontend | `http://workforce-optimizer-frontend:3000` |
| Web Portal | `http://web-portal:3001` |

### External URLs (Ingress)

| Service | External URL |
|---------|--------------|
| Web Portal | `https://mi-hub.dev` |
| Scheduler | `https://scheduler.mi-hub.dev` |
| Workforce Optimizer | `https://workforce.mi-hub.dev` |
| Agents Portal | `https://portal.mi-hub.dev` |
| Agents Portal UI | `https://portal-ui.mi-hub.dev` |
| Agents Runtime | `https://runtime.mi-hub.dev` |
| Agents Service | `https://agents.mi-hub.dev` |
| Artifacts Service | `https://artifacts.mi-hub.dev` |
| Knowledge Service | `https://knowledge.mi-hub.dev` |

### Databases

| Service | Database Name |
|---------|---------------|
| Plant | `mi_plant` |
| User Management | `mi_user_management` |
| Scheduler | `mi_scheduler` |
| Workforce Optimizer | `mi_workforce_optimizer` |
| Data Product Hub | `mi_dph` |
| Scheduling Engine Core | `scheduling_engine_core` |
| Agents Portal | `genaix_portal` |
| Agents Service | `genaix_agents` |
| Artifacts Service | `genaix_artifacts` |
| Knowledge Service | `genaix_knowledge` |
| Agents Runtime | `genaix_runtime` |

---

## 13. Automation Scripts

> **WHY USE SCRIPTS?** Scripts automate repetitive tasks and ensure consistency.
> Instead of typing 12 commands manually, run one script that does it all.

### How to Execute Scripts

All scripts should be run from the **workspace root directory**:

```bash
cd /home/ivimehaj/projects/Ariston/MI.SA
```

#### Making Scripts Executable

Before running a script for the first time, make it executable:

```bash
# Make a single script executable
chmod +x scripts/deploy-all.sh

# Or make all scripts executable at once
chmod +x scripts/*.sh
```

#### Running Scripts

```bash
# Standard way: use ./
./scripts/deploy-all.sh

# Or use bash explicitly
bash scripts/deploy-all.sh

# Pass arguments (e.g., image tag)
./scripts/build-all-images.sh v1.0.0
```

### Available Scripts

Create these scripts in `/home/ivimehaj/projects/Ariston/MI.SA/scripts/`:

#### 1. `check-status.sh` - Check Infrastructure Status

```bash
#!/bin/bash
# File: scripts/check-status.sh
# Purpose: Quick status check of all Azure resources
# Run: ./scripts/check-status.sh

echo "============================================"
echo "MI.Hub Infrastructure Status Check"
echo "============================================"

echo -e "\nğŸ“¦ ACR Images:"
az acr repository list --name acr31314402uksnpdmihacr --output table 2>/dev/null || echo "âŒ Cannot access ACR"

echo -e "\nğŸ¯ AKS Nodes:"
kubectl get nodes 2>/dev/null || echo "âŒ Cannot access AKS"

echo -e "\nğŸ“Š Deployed Pods:"
kubectl get pods -n mi-hub-dev --no-headers 2>/dev/null | wc -l | xargs -I {} echo "{} pods running"
kubectl get pods -n mi-hub-dev 2>/dev/null || echo "âŒ Namespace not found"

echo -e "\nâœ… Status check complete"
```

#### 2. `build-all-images.sh` - Build All Docker Images

```bash
#!/bin/bash
# File: scripts/build-all-images.sh
# Purpose: Build all Docker images for all services
# Run: ./scripts/build-all-images.sh [TAG]
# Example: ./scripts/build-all-images.sh v1.0.0

set -e  # Exit on any error

ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="${1:-latest}"
ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"

echo "============================================"
echo "Building all images with tag: ${TAG}"
echo "Working directory: ${ROOT_DIR}"
echo "============================================"

cd "${ROOT_DIR}"

# Services that build from their own directory
declare -A SERVICES=(
  ["scheduler-backend"]="MI.SA.Scheduler/backend/Dockerfile:MI.SA.Scheduler/backend"
  ["scheduler-frontend"]="MI.SA.Scheduler/frontend/Dockerfile.dev:MI.SA.Scheduler/frontend"
  ["workforce-optimizer-backend"]="MI.SA.WorkforceOptimizer/backend/Dockerfile:MI.SA.WorkforceOptimizer/backend"
  ["workforce-optimizer-frontend"]="MI.SA.WorkforceOptimizer/frontend/Dockerfile.dev:MI.SA.WorkforceOptimizer/frontend"
  ["web-portal"]="MI.WebPortal/frontend/Dockerfile:MI.WebPortal/frontend"
  ["agents-portal-backend"]="MI.AgentsPortal/backend/Dockerfile:MI.AgentsPortal/backend"
  ["agents-portal-ui"]="MI.AgentsPortal/ui/Dockerfile:MI.AgentsPortal/ui"
  ["agents-runtime"]="MI.AgentsRuntime/Dockerfile.example:MI.AgentsRuntime"
  ["scheduling-engine-core"]="MI.SA.SchedulingEngineCore/Dockerfile:MI.SA.SchedulingEngineCore"
)

for name in "${!SERVICES[@]}"; do
  IFS=':' read -r dockerfile context <<< "${SERVICES[$name]}"
  echo ""
  echo "========================================="
  echo "Building: ${name}"
  echo "Dockerfile: ${dockerfile}"
  echo "Context: ${context}"
  echo "========================================="
  docker build -t "${ACR_NAME}/${name}:${TAG}" -f "${dockerfile}" "${context}"
done

# CoreServices (need root context because they reference ../MI.AgentsRuntime)
echo ""
echo "========================================="
echo "Building CoreServices (from root context)"
echo "========================================="
docker build -t "${ACR_NAME}/agents-service:${TAG}" -f MI.AgentsCoreServices/services/agents_service/Dockerfile .
docker build -t "${ACR_NAME}/artifacts-service:${TAG}" -f MI.AgentsCoreServices/services/artifacts_service/Dockerfile .
docker build -t "${ACR_NAME}/knowledge-service:${TAG}" -f MI.AgentsCoreServices/services/knowledge_service/Dockerfile .

echo ""
echo "============================================"
echo "âœ… All images built successfully!"
echo "Tag: ${TAG}"
echo "============================================"
```

#### 3. `push-all-images.sh` - Push All Images to ACR

```bash
#!/bin/bash
# File: scripts/push-all-images.sh
# Purpose: Push all Docker images to Azure Container Registry
# Run: ./scripts/push-all-images.sh [TAG]
# Prerequisite: Run 'az acr login --name acr31314402uksnpdmihacr' first

set -e

ACR_NAME="acr31314402uksnpdmihacr.azurecr.io"
TAG="${1:-latest}"

IMAGES=(
  "scheduler-backend"
  "scheduler-frontend"
  "workforce-optimizer-backend"
  "workforce-optimizer-frontend"
  "web-portal"
  "agents-portal-backend"
  "agents-portal-ui"
  "agents-runtime"
  "agents-service"
  "artifacts-service"
  "knowledge-service"
  "scheduling-engine-core"
)

echo "============================================"
echo "Pushing all images with tag: ${TAG}"
echo "============================================"

for image in "${IMAGES[@]}"; do
  echo "Pushing: ${image}:${TAG}"
  docker push "${ACR_NAME}/${image}:${TAG}"
done

echo ""
echo "============================================"
echo "âœ… All images pushed successfully!"
echo "============================================"
```

#### 4. `deploy-all.sh` - Deploy All Services

```bash
#!/bin/bash
# File: scripts/deploy-all.sh
# Purpose: Deploy all services to Kubernetes in correct dependency order
# Run: ./scripts/deploy-all.sh
# Prerequisite: Images must exist in ACR, secrets must be configured

set -e

NAMESPACE="mi-hub-dev"
ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"

echo "============================================"
echo "Deploying all services to namespace: ${NAMESPACE}"
echo "Working directory: ${ROOT_DIR}"
echo "============================================"

cd "${ROOT_DIR}"

# Services in DEPENDENCY ORDER - this order matters!
SERVICES=(
  # Layer 1: Core backends (no dependencies)
  "MI.Backend.Plant"
  "MI.Backend.UserManagement"
  "MI.Backend.AgenticSystem"
  "MI.Backend.AgenticScheduling"
  
  # Layer 2: Data services (depend on backends)
  "MI.DataProductHub"
  "MI.SA.SchedulingEngineCore"
  
  # Layer 3: AI services (depend on data)
  "MI.AgentsRuntime"
  "MI.AgentsCoreServices"
  
  # Layer 4: Application services (depend on AI)
  "MI.AgentsPortal"
  "MI.SA.Scheduler"
  "MI.SA.WorkforceOptimizer"
  
  # Layer 5: Frontend (depends on all backends)
  "MI.WebPortal"
)

DEPLOYED=0
SKIPPED=0

for service in "${SERVICES[@]}"; do
  KUBE_DIR="${ROOT_DIR}/${service}/kube/development"
  
  if [ -d "$KUBE_DIR" ]; then
    echo ""
    echo "========================================="
    echo "Deploying: ${service}"
    echo "========================================="
    kubectl apply -k "${KUBE_DIR}"
    DEPLOYED=$((DEPLOYED + 1))
    
    # Wait a bit between deployments to let pods start
    sleep 3
  else
    echo "SKIP: ${service} (no kube/development directory)"
    SKIPPED=$((SKIPPED + 1))
  fi
done

echo ""
echo "============================================"
echo "Deployment Summary"
echo "============================================"
echo "Deployed: ${DEPLOYED} services"
echo "Skipped:  ${SKIPPED} services"
echo ""
echo "Check status with:"
echo "  kubectl get pods -n ${NAMESPACE}"
echo ""
echo "Follow logs with:"
echo "  kubectl logs -f -l app=<app-name> -n ${NAMESPACE}"
```

#### 5. `delete-all.sh` - Remove All Deployments

```bash
#!/bin/bash
# File: scripts/delete-all.sh
# Purpose: Remove all deployments from Kubernetes
# Run: ./scripts/delete-all.sh
# WARNING: This will delete all services!

set -e

NAMESPACE="mi-hub-dev"
ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"

echo "============================================"
echo "âš ï¸  WARNING: This will delete all services!"
echo "Namespace: ${NAMESPACE}"
echo "============================================"
read -p "Are you sure? (y/N): " confirm

if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
  echo "Aborted."
  exit 0
fi

cd "${ROOT_DIR}"

SERVICES=(
  "MI.WebPortal"
  "MI.SA.WorkforceOptimizer"
  "MI.SA.Scheduler"
  "MI.AgentsPortal"
  "MI.AgentsCoreServices"
  "MI.AgentsRuntime"
  "MI.SA.SchedulingEngineCore"
  "MI.DataProductHub"
  "MI.Backend.AgenticScheduling"
  "MI.Backend.AgenticSystem"
  "MI.Backend.UserManagement"
  "MI.Backend.Plant"
)

for service in "${SERVICES[@]}"; do
  KUBE_DIR="${ROOT_DIR}/${service}/kube/development"
  
  if [ -d "$KUBE_DIR" ]; then
    echo "Deleting: ${service}"
    kubectl delete -k "${KUBE_DIR}" --ignore-not-found
  fi
done

echo ""
echo "âœ… All services deleted"
```

### Using Scripts: Complete Workflow Example

```bash
# 1. Navigate to workspace root
cd /home/ivimehaj/projects/Ariston/MI.SA

# 2. Make scripts executable (one time only)
chmod +x scripts/*.sh

# 3. Check current status
./scripts/check-status.sh

# 4. Login to Azure (if not already)
az login
az acr login --name acr31314402uksnpdmihacr
az aks get-credentials --resource-group rg-31314402-UKS-mihazks --name aksnpdmih

# 5. Build all images
./scripts/build-all-images.sh latest
# Or with a version tag:
./scripts/build-all-images.sh v1.0.0

# 6. Push all images
./scripts/push-all-images.sh latest

# 7. Deploy all services
./scripts/deploy-all.sh

# 8. Verify deployment
kubectl get pods -n mi-hub-dev

# 9. (Optional) Clean up everything
./scripts/delete-all.sh
```

---

## Quick Reference Commands

> **All commands run from:** `/home/ivimehaj/projects/Ariston/MI.SA` (workspace root)

```bash
# ============================================================================
# STEP 1: Initial Setup (run once per session)
# ============================================================================
az login
az aks get-credentials --resource-group rg-31314402-UKS-mihazks --name aksnpdmih
az acr login --name acr31314402uksnpdmihacr

# ============================================================================
# STEP 2: Check Status (what exists already?)
# ============================================================================
az acr repository list --name acr31314402uksnpdmihacr -o table  # Images in ACR
kubectl get pods -n mi-hub-dev                                    # Running pods
kubectl get svc -n mi-hub-dev                                     # Services

# ============================================================================
# STEP 3: Build & Push (single service example)
# ============================================================================
docker build -t acr31314402uksnpdmihacr.azurecr.io/scheduler-backend:latest \
  -f MI.SA.Scheduler/backend/Dockerfile MI.SA.Scheduler/backend
docker push acr31314402uksnpdmihacr.azurecr.io/scheduler-backend:latest

# ============================================================================
# STEP 4: Deploy (single service)
# ============================================================================
kubectl apply -k MI.SA.Scheduler/kube/development

# ============================================================================
# STEP 5: Monitor
# ============================================================================
kubectl get pods -n mi-hub-dev                                    # List all pods
kubectl get pods -n mi-hub-dev -w                                 # Watch pods (live)
kubectl logs -f -l app=scheduler-backend -n mi-hub-dev            # Follow logs
kubectl describe pod <pod-name> -n mi-hub-dev                     # Debug pod issues

# ============================================================================
# STEP 6: Access Services
# ============================================================================
kubectl port-forward svc/scheduler-backend 8000:8000 -n mi-hub-dev
# Then open: http://localhost:8000/docs

# ============================================================================
# CLEANUP: Remove deployment
# ============================================================================
kubectl delete -k MI.SA.Scheduler/kube/development
```

---

## Next Steps

After successful deployment:

1. **DNS Configuration**
   - Get ingress controller external IP: `kubectl get svc -n ingress-nginx`
   - Configure `*.mi-hub.dev` A record to point to that IP

2. **TLS Certificates**
   - Install cert-manager: `kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml`
   - Configure Let's Encrypt ClusterIssuer

3. **CI/CD Pipeline**
   - Set up Azure DevOps or Bitbucket pipelines
   - Automate: build â†’ push â†’ deploy on merge to main

4. **Monitoring**
   - Deploy Prometheus for metrics
   - Deploy Grafana for dashboards
   - Configure Azure Monitor integration

5. **Logging**
   - Configure Azure Log Analytics workspace
   - Or deploy ELK (Elasticsearch, Logstash, Kibana) stack

6. **Backup**
   - Configure PostgreSQL automated backups
   - Set up Velero for Kubernetes backup
